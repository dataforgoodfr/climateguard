{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915334d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_studio_id': 1739,\n",
       " 'id': '0dbeca955359710bd676973cc0bfdb5ec2151ff6d7cc0f6a08a88187840cebb7',\n",
       " 'day': 13,\n",
       " 'year': 2024,\n",
       " 'month': 11,\n",
       " 'start': '2024-11-13T08:52:00',\n",
       " 'channel': 'rmc',\n",
       " 'plaintext': \"et on est obligé de baisser nos émissions de carbone à moins de vouloir euh avoir des inondations à valence tous les week <unk> end inondations annuelles et ben oui précisément le rapport entre les émissions carbone et les catastrophes naturelles il est il est tout droit il est direct et vu que les jets privés font beaucoup d' émissions de carbone et c' est un lien si elle vient de dire c' est physiquement rien scientifique ça n' a rien à voir si le réchauffement des eaux c' est pas lié est aux pas lié avions d' aux affaires aux freiner avions le d' réchauffement affaires des freiner eaux le ces réchauffement livres des eaux c' est lié au réchauffement climatique il est pinos il est lié aux émissions carbones oui mais tout est peanuts donc si on considère que toute épinette on fait rien et du coup on a des gens comme toi qui euh qui détruisent la planète civilisation eux donc je vais quand même je vais quand même terminé euh en fait le problème avec le carbone c' est que si on considère que tout et rien on tape on baisse nulle part en fait c' est donc on fait un choix et c' est le choix font que actuellement nos qui gouvernants est font de actuellement privilégier qui l' est économie de sur l' écologie c' est un choix politique le problème de choix politique c' est que déjà euh il permet de maintenir une espèce de stabilité économique peut fixer va on l' a vue il va y avoir beaucoup de faillites cette année l' un des dix réalisations en europe c' est un gros euh fly pour eux donc en plus déjà ça marche pas et en plus ça fait que on va se taper des catastrophes naturelles régulièrement et des sécheresses des pénuries n' oublions pas que l' économie ne marche que grâce aux ressources naturelles en en vous vendez vous pas vendez euh des fruits et légumes si vous ne pouvez pas cultiver fruits et légumes donc en fait si on veut éviter une récession aussi mondiale et planétaire on doit baisser les émissions carbone donc je veux bien que ce soit par l' aviation d' affaires je veux bien que ce soit pas j euh la mondialisation à un moment et on doit aller quelque part dans le sens de flora j' entendais un philosophe hier qui disait on pleure sur les inondations et les morts de valence mais on continue à prendre l' avion pour aller en grèce c' est quoi le rapport olivier sur les villes\",\n",
       " 'model_name': 'ft:gpt-4o-mini-2024-07-18:personal::B1xWiJRm',\n",
       " 'channel_name': 'rmc',\n",
       " 'model_reason': '',\n",
       " 'model_result': 0,\n",
       " 'channel_title': 'RMC',\n",
       " 'url_mediatree': 'https://keywords.mediatree.fr/player/?fifo=rmc&start_cts=1731487799&end_cts=1731488161&position_cts=1731487920',\n",
       " 'channel_program': 'Les grandes gueules',\n",
       " 'plaintext_whisper': \"de baisser nos émissions carbone, à moins de vouloir avoir des inondations à valence que tous les mois ou tous les semaines. C'est les jets privés qui font les inondations. Oui précisément, le rapport entre les émissions carbone et les catastrophes naturelles, il est tout droit, il est direct. Et vu que les jets privés font beaucoup d'émissions carbone, c'est un lien... C'est comique ce que tu viens de dire. Ce n'est pas comique, c'est scientifique. Ça n'a rien à voir, c'est le réchauffement des eaux, ce n'est pas lié aux avions d'affaires. Mais le réchauffement des eaux, c'est lié au réchauffement climatique. Le réchauffement climatique, il est lié aux émissions carbone. Oui mais tout est peanuts, donc si on considère que tout est peanuts, on ne fait rien. Et du coup, on a des gens comme toi qui détruisent la planète et l'avenir des civilisations. Donc je vais quand même terminer. En fait, le problème avec le carbone, c'est que si on considère que tout est rien, on baisse nulle part en fait. Et donc on fait un choix, et c'est le choix que nos gouvernants font actuellement, qui est de privilégier l'économie sur l'écologie. C'est un choix politique. Le problème de ce choix politique, c'est que déjà, il permet de maintenir une espèce de stabilité économique un peu fictive. On l'a vu, il va y avoir beaucoup de faillites cette année. La désindustrialisation en Europe, c'est un gros fléau. Donc en plus, déjà, ça ne marche pas. Et en plus, ça fait qu'on va se taper des catastrophes naturelles régulièrement et des sécheresses, des pénuries. N'oublions pas que l'économie ne marche que grâce aux ressources naturelles. Vous ne vendez pas des fruits et légumes si vous ne pouvez pas cultiver de fruits et légumes. Donc en fait, si on veut éviter une récession aussi mondiale et planétaire, on doit baisser les émissions de carbone. Donc je veux bien que ce ne soit pas l'aviation d'affaires, je veux bien que ce ne soit pas la mondialisation. À un moment, on doit aller quelque part. Pour aller dans le sens de Flora, j'entendais un philosophe hier qui disait on pleure sur les inondations et les morts de Valence, mais on continue à prendre l'avion pour aller en Grèce.\",\n",
       " 'channel_program_type': '',\n",
       " 'week_number': 46,\n",
       " 'misinformation': False,\n",
       " 'cards_claims': [],\n",
       " 'misinformation_claims': [],\n",
       " 'comments': []}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    " \n",
    "# Dataset id from huggingface.co/dataset\n",
    "dataset_id = \"DataForGood/climateguard\"\n",
    " \n",
    "# Load raw dataset\n",
    "train_dataset = load_dataset(dataset_id, split='train')\n",
    "\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "split_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8d8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=128,\n",
    ")\n",
    "# chunks = splitter.split_text(record[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bae8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "\n",
    "for record in train_dataset:\n",
    "    if record[\"misinformation_claims\"]:\n",
    "        for claim in record[\"misinformation_claims\"]:\n",
    "            for label in claim[\"labels\"]:\n",
    "                records.append({\n",
    "                    \"text\": claim[\"text\"],\n",
    "                    \"label\": 1\n",
    "                })\n",
    "    else:\n",
    "        chunks = splitter.split_text(record[\"plaintext\"])\n",
    "        records.append({\n",
    "            \"text\": random.choice(chunks),\n",
    "            \"label\": int(record[\"misinformation\"]),\n",
    "        })\n",
    "\n",
    "claims_dataset = Dataset.from_pandas(pd.DataFrame.from_records(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a22e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"avion en fait ils pointent à chaque fois des responsabilités en fait c' est peut être tout simplement l' arrêt la responsabilité d' une personne alors que nous nous passons notre temps à noyer les responsabilités individuelles c' est à dire que si ça se passe en france on a expliqué que c' est parce que le contrôleur travaille trop et que les conditions de travail ne sont pas ben khemis seck et que tout d' un coup le syndicat va monter au créneau pour dire ben oui réagirait chez nous le président ne réagit pas à jouer il y aurait une enquête yves <unk> les conclusions vous n' en nommer que je veux vous dire si je devais gyrophare dans l' état d' esprit de la france qui est le nôtre c' est que tout est inversé ça servirait la cause de ceux qui excite ben oui écoutez la la charge de travail vous rendez pas compte qu' il y a sur ces gens là euh aux médias les treize juges dans tous les domaines de la france pointer répugne les pointer responsabilités individuelles les derrière responsabilités des individuelles abstractions un qui ne permettent jamais de mettre en cause véritablement les transgressions qui ont été commis c' est une maladie je l' ai vu dans le judiciaire et c' est encore vrai là c' est ce qui me plaît chez trump on en sait steam philippe bilger bill gère prost ils parlent lorsqu' ils parlent lorsqu' il arrive que nous soyons d' accord mais bien sûr mais on sûr laisse mais le on le l' plus est souvent le plus en souvent oui un oui <unk> est pas moi non vous ne trouvez pas que c' est juste moi j' ai l' impression exactement inverses que dès qu' il y a une catastrophe on cherche un coupable non mais s' est jamais la fatalité c' est le réchauffement climatique c' est un tel c' est du pneu ces tarte moles y a toujours un responsable c' est vrai mais on a on le nomme jamais mais il ne s' appelle pas loïc quand même s' il s' appelle comment vous pouvez on peut tous s' appliquent dans notre robert\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = claims_dataset.train_test_split(test_size=0.1)\n",
    "split_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa620a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5399ba56232c4140856636e2f1e88e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3659897e57334f23a22bb910ce2a7085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e24640e3d845a79974523af18e61ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7f4fb2c0ef494a88a3576d37dbfc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['label_studio_id', 'id', 'day', 'year', 'month', 'start', 'channel', 'model_name', 'channel_name', 'model_reason', 'model_result', 'channel_title', 'url_mediatree', 'channel_program', 'plaintext_whisper', 'channel_program_type', 'week_number', 'misinformation', 'cards_claims', 'misinformation_claims', 'comments', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    " \n",
    "# Model id to load the tokenizer\n",
    "model_id = \"almanach/camembertav2-base\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "def format_labels(example):\n",
    "    return {\"labels\": str(int(example[\"misinformation\"]))}\n",
    " \n",
    "# Tokenize dataset\n",
    "# if \"misinformation\" in split_dataset[\"train\"].features.keys():\n",
    "#     split_dataset =  split_dataset.rename_column(\"misinformation\", \"labels\") # to match Trainer\n",
    "tokenized_dataset = split_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.map(format_labels, batched=False)\n",
    " \n",
    "tokenized_dataset[\"train\"].features.keys()\n",
    "# dict_keys(['labels', 'input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "505f3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_studio_id': Value(dtype='int64', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'day': Value(dtype='int64', id=None),\n",
       " 'year': Value(dtype='int64', id=None),\n",
       " 'month': Value(dtype='int64', id=None),\n",
       " 'start': Value(dtype='string', id=None),\n",
       " 'channel': Value(dtype='string', id=None),\n",
       " 'model_name': Value(dtype='string', id=None),\n",
       " 'channel_name': Value(dtype='string', id=None),\n",
       " 'model_reason': Value(dtype='string', id=None),\n",
       " 'model_result': Value(dtype='int64', id=None),\n",
       " 'channel_title': Value(dtype='string', id=None),\n",
       " 'url_mediatree': Value(dtype='string', id=None),\n",
       " 'channel_program': Value(dtype='string', id=None),\n",
       " 'plaintext_whisper': Value(dtype='string', id=None),\n",
       " 'channel_program_type': Value(dtype='string', id=None),\n",
       " 'week_number': Value(dtype='int64', id=None),\n",
       " 'misinformation': Value(dtype='bool', id=None),\n",
       " 'cards_claims': [{'labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'text': Value(dtype='string', id=None)}],\n",
       " 'misinformation_claims': [{'labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'text': Value(dtype='string', id=None)}],\n",
       " 'comments': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba950a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model id to load the tokenizer\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    " \n",
    "# Prepare model labels - useful for inference\n",
    "labels = list(set(tokenized_dataset[\"train\"][\"labels\"]))\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    " \n",
    "# Download the model from huggingface.co/models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d1f223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       input_ids\n",
       "count      643.0\n",
       "mean       658.0\n",
       "std          0.0\n",
       "min        658.0\n",
       "25%        658.0\n",
       "50%        658.0\n",
       "75%        658.0\n",
       "max        658.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].to_pandas()[[\"input_ids\"]].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4bb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    score = f1_score(\n",
    "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
    "        )\n",
    "    return {\"f1\": float(score) if score == 1 else score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818189b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModernBERT-base-climateguard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     hub_token\u001b[38;5;241m=\u001b[39mHfFolder\u001b[38;5;241m.\u001b[39mget_token(),\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Create a Trainer instance\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# {'train_runtime': 3642.7783, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.04, 'train_loss': 0.535627057634551, 'epoch': 5.0}\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/d4g/climateguard/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/d4g/climateguard/.venv/lib/python3.12/site-packages/transformers/trainer.py:471\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/Documents/d4g/climateguard/.venv/lib/python3.12/site-packages/transformers/trainer.py:5176\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires accelerate>1.3.0 to use Tensor Parallelism.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5175\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5177\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/Documents/d4g/climateguard/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:547\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdaa\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    546\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    548\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# FSDP2 doesn't use ShardedGradScaler, don't want to modify `get_grad_scaler`, rather create a simple utility\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    " \n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"ModernBERT-base-climateguard\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "        num_train_epochs=5,\n",
    "    bf16=False, # bfloat16 training \n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch_fused\", # improved optimizer \n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    " \n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "# {'train_runtime': 3642.7783, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.04, 'train_loss': 0.535627057634551, 'epoch': 5.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daecc913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-14.3-arm64-arm-64bit'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab28a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/jjyv0bm5597_x15dvpxnsmrc0000gn/T/ipykernel_49241/908756464.py:3: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859048f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
