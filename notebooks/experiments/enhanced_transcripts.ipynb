{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisperization\n",
    "\n",
    "Using faster-whisper https://github.com/SYSTRAN/faster-whisper/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b30f9f0f3994151a048fe8f9050a1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   2%|2         | 62.9M/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_audio_path = \"../../../MediatreeMP3/testAudio\"\n",
    "\n",
    "train_audio_files = [file for file in os.listdir(train_audio_path) if file.endswith(\".mp3\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faster_whisper:Processing audio with duration 02:00.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr - 1\n",
      "[0.00s -> 2.42s]  Et quand il y a du soleil, on n'a pas besoin d'énergie, en fait.\n",
      "[3.00s -> 5.20s]  On n'a pas besoin d'énergie quand il y a du soleil dans la journée.\n",
      "[5.34s -> 9.92s]  On a besoin, je termine, d'énergie quand le soleil se couche.\n",
      "[10.56s -> 11.44s]  Donc, en fait, ça ne sert à rien.\n",
      "[12.78s -> 15.46s]  Donc, une fois qu'on a dit ça, on passe à autre chose.\n",
      "[15.78s -> 17.08s]  Des capacités de stockage.\n",
      "[17.58s -> 18.90s]  Mais on ne stocke pas l'énergie solaire.\n",
      "[18.90s -> 22.18s]  Le nucléaire permettra pas de répondre à tous nos besoins.\n",
      "[22.38s -> 23.00s]  C'est ça que je veux dire.\n",
      "[23.54s -> 25.88s]  Philippe, on ne stocke pas l'énergie solaire.\n",
      "[26.58s -> 27.42s]  On ne stocke pas l'énergie solaire.\n",
      "[27.42s -> 28.72s]  Et on fait comment, alors ?\n",
      "[28.72s -> 32.44s]  On fait, effectivement, on privilégie le nucléaire,\n",
      "[32.50s -> 33.42s]  ce qu'on est en train de faire, d'ailleurs.\n",
      "[33.82s -> 37.80s]  Mais ça ne suffit pas à découvrir les besoins du pays.\n",
      "[38.00s -> 39.06s]  Ce n'est pas juste quelque chose de technicien.\n",
      "[39.06s -> 40.28s]  C'est aussi une question d'esthétique.\n",
      "[40.98s -> 43.22s]  Écoutez, Paul Melun, parce qu'il a fait une petite vidéo,\n",
      "[43.38s -> 45.38s]  coup de gueule du jour, au détour d'une promenade en forêt,\n",
      "[45.44s -> 47.42s]  je tombe sur un immense parc éolien.\n",
      "[47.42s -> 49.16s]  Voilà, la France défigurée.\n",
      "[49.36s -> 51.24s]  Vous vous souvenez de cette émission ?\n",
      "[51.24s -> 52.40s]  Comment s'appelait-il ?\n",
      "[52.40s -> 53.82s]  La France défigurée.\n",
      "[55.02s -> 56.92s]  Écoutez, monsieur Paul Melun.\n",
      "[58.72s -> 62.26s]  Bonjour à tous, je suis en direct des Deux-Sèvres.\n",
      "[62.38s -> 64.92s]  Vous voyez, je fais ma promenade, j'ai pris ma carte, je me promène.\n",
      "[65.04s -> 70.18s]  Il y a derrière moi des beaux paysages, avec des forêts, des bosquets.\n",
      "[70.40s -> 71.56s]  Voilà, c'est assez agréable.\n",
      "[71.66s -> 74.48s]  Et puis là, vous voyez, au bout de quelques minutes de marche,\n",
      "[74.56s -> 76.70s]  je suis tombé sur quelque chose qui me désespère beaucoup,\n",
      "[76.70s -> 79.62s]  qui me déplait fortement.\n",
      "[79.82s -> 81.96s]  C'est un parc éolien qui est absolument immense,\n",
      "[82.88s -> 86.04s]  avec des piliers de plusieurs dizaines et dizaines de mètres de haut.\n",
      "[86.16s -> 86.60s]  Vous voyez, ils sont là.\n",
      "[87.86s -> 88.70s]  Je suis complètement...\n",
      "[88.84s -> 92.26s]  Il y a un auto-tour, vraiment partout, partout, partout.\n",
      "[92.42s -> 95.86s]  Vous voyez, on peut faire le tour à 360 degrés.\n",
      "[96.80s -> 97.96s]  Il y en a partout.\n",
      "[98.66s -> 99.40s]  Ça fait un bruit.\n",
      "[100.22s -> 104.02s]  On a l'impression d'être un mélange entre un aéroport et une autoroute,\n",
      "[104.80s -> 105.70s]  si vous voulez entendre.\n",
      "[111.84s -> 113.44s]  Voilà, c'est l'écologie bruyante.\n",
      "[114.16s -> 117.50s]  En passant sur le chemin, je suis tombé sur des oiseaux qui ont été déchiquetés.\n",
      "[117.50s -> 119.56s]  Les cadavres par terre.\n",
      "[119.92s -> 120.00s]  Voilà.\n",
      "CPU times: user 10min 38s, sys: 1min 2s, total: 11min 40s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "first_file = train_audio_path + \"/\" + train_audio_files[1]\n",
    "\n",
    "segments, info = model.transcribe(first_file, language=\"fr\") # beam_size=5, , condition_on_previous_text=False\n",
    "print(f\"{info.language} - {info.language_probability}\")\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whisper_transcript(file_path, model):\n",
    "    \"\"\"Call whisper to get transcription\"\"\"\n",
    "    segments, info = model.transcribe(file_path, language=\"fr\") # beam_size=5, , condition_on_previous_text=False\n",
    "    return \" \".join(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et quand il y a du soleil, on n'a pas besoin d'énergie, en fait. On n'a pas besoin d'énergie quand il y a du soleil dans la journée. On a besoin, je termine, d'énergie quand le soleil se couche. Donc, en fait, ça ne sert à rien. Donc, une fois qu'on a dit ça, on passe à autre chose. Des capacités de stockage. Mais on ne stocke pas l'énergie solaire. Le nucléaire permettra pas de répondre à tous nos besoins. C'est ça que je veux dire. Philippe, on ne stocke pas l'énergie solaire. On ne stocke pas l'énergie solaire. Et on fait comment, alors ? On fait, effectivement, on privilégie le nucléaire, ce qu'on est en train de faire, d'ailleurs. Mais ça ne suffit pas à découvrir les besoins du pays. Ce n'est pas juste quelque chose de technicien. C'est aussi une question d'esthétique. Écoutez, Paul Melun, parce qu'il a fait une petite vidéo, coup de gueule du jour, au détour d'une promenade en forêt, je tombe sur un immense parc éolien. Voilà, la France défigurée. Vous vous souvenez de cette émission ? Comment s'appelait-il ? La France défigurée. Écoutez, monsieur Paul Melun. Bonjour à tous, je suis en direct des Deux-Sèvres. Vous voyez, je fais ma promenade, j'ai pris ma carte, je me promène. Il y a derrière moi des beaux paysages, avec des forêts, des bosquets. Voilà, c'est assez agréable. Et puis là, vous voyez, au bout de quelques minutes de marche, je suis tombé sur quelque chose qui me désespère beaucoup, qui me déplait fortement. C'est un parc éolien qui est absolument immense, avec des piliers de plusieurs dizaines et dizaines de mètres de haut. Vous voyez, ils sont là. Je suis complètement... Il y a un auto-tour, vraiment partout, partout, partout. Vous voyez, on peut faire le tour à 360 degrés. Il y en a partout. Ça fait un bruit. On a l'impression d'être un mélange entre un aéroport et une autoroute, si vous voulez entendre. Voilà, c'est l'écologie bruyante. En passant sur le chemin, je suis tombé sur des oiseaux qui ont été déchiquetés. Les cadavres par terre. Voilà.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Et quand il y a du soleil, on n'a pas besoin d'énergie, en fait.\n",
    "[3.00s -> 5.20s]  On n'a pas besoin d'énergie quand il y a du soleil dans la journée.\n",
    "[5.34s -> 9.92s]  On a besoin, je termine, d'énergie quand le soleil se couche.\n",
    "[10.56s -> 11.44s]  Donc, en fait, ça ne sert à rien.\n",
    "[12.78s -> 15.46s]  Donc, une fois qu'on a dit ça, on passe à autre chose.\n",
    "[15.78s -> 17.08s]  Des capacités de stockage.\n",
    "[17.58s -> 18.90s]  Mais on ne stocke pas l'énergie solaire.\n",
    "[18.90s -> 22.18s]  Le nucléaire permettra pas de répondre à tous nos besoins.\n",
    "[22.38s -> 23.00s]  C'est ça que je veux dire.\n",
    "[23.54s -> 25.88s]  Philippe, on ne stocke pas l'énergie solaire.\n",
    "[26.58s -> 27.42s]  On ne stocke pas l'énergie solaire.\n",
    "[27.42s -> 28.72s]  Et on fait comment, alors ?\n",
    "[28.72s -> 32.44s]  On fait, effectivement, on privilégie le nucléaire,\n",
    "[32.50s -> 33.42s]  ce qu'on est en train de faire, d'ailleurs.\n",
    "[33.82s -> 37.80s]  Mais ça ne suffit pas à découvrir les besoins du pays.\n",
    "[38.00s -> 39.06s]  Ce n'est pas juste quelque chose de technicien.\n",
    "[39.06s -> 40.28s]  C'est aussi une question d'esthétique.\n",
    "[40.98s -> 43.22s]  Écoutez, Paul Melun, parce qu'il a fait une petite vidéo,\n",
    "[43.38s -> 45.38s]  coup de gueule du jour, au détour d'une promenade en forêt,\n",
    "[45.44s -> 47.42s]  je tombe sur un immense parc éolien.\n",
    "[47.42s -> 49.16s]  Voilà, la France défigurée.\n",
    "[49.36s -> 51.24s]  Vous vous souvenez de cette émission ?\n",
    "[51.24s -> 52.40s]  Comment s'appelait-il ?\n",
    "[52.40s -> 53.82s]  La France défigurée.\n",
    "[55.02s -> 56.92s]  Écoutez, monsieur Paul Melun.\n",
    "[58.72s -> 62.26s]  Bonjour à tous, je suis en direct des Deux-Sèvres.\n",
    "[62.38s -> 64.92s]  Vous voyez, je fais ma promenade, j'ai pris ma carte, je me promène.\n",
    "[65.04s -> 70.18s]  Il y a derrière moi des beaux paysages, avec des forêts, des bosquets.\n",
    "[70.40s -> 71.56s]  Voilà, c'est assez agréable.\n",
    "[71.66s -> 74.48s]  Et puis là, vous voyez, au bout de quelques minutes de marche,\n",
    "[74.56s -> 76.70s]  je suis tombé sur quelque chose qui me désespère beaucoup,\n",
    "[76.70s -> 79.62s]  qui me déplait fortement.\n",
    "[79.82s -> 81.96s]  C'est un parc éolien qui est absolument immense,\n",
    "[82.88s -> 86.04s]  avec des piliers de plusieurs dizaines et dizaines de mètres de haut.\n",
    "[86.16s -> 86.60s]  Vous voyez, ils sont là.\n",
    "[87.86s -> 88.70s]  Je suis complètement...\n",
    "[88.84s -> 92.26s]  Il y a un auto-tour, vraiment partout, partout, partout.\n",
    "[92.42s -> 95.86s]  Vous voyez, on peut faire le tour à 360 degrés.\n",
    "[96.80s -> 97.96s]  Il y en a partout.\n",
    "[98.66s -> 99.40s]  Ça fait un bruit.\n",
    "[100.22s -> 104.02s]  On a l'impression d'être un mélange entre un aéroport et une autoroute,\n",
    "[104.80s -> 105.70s]  si vous voulez entendre.\n",
    "[111.84s -> 113.44s]  Voilà, c'est l'écologie bruyante.\n",
    "[114.16s -> 117.50s]  En passant sur le chemin, je suis tombé sur des oiseaux qui ont été déchiquetés.\n",
    "[117.50s -> 119.56s]  Les cadavres par terre.\n",
    "[119.92s -> 120.00s]  Voilà.\n",
    "\"\"\"\n",
    "def remove_text_in_brackets(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[\\n\\t]', '', text)\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    return text\n",
    "print(remove_text_in_brackets(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced_transcripts.ipynb\t  mediatree_single_prompt_prediction.ipynb\n",
      "excel_deliverable_december.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob(\"../../data/results/test/*\")\n",
    "\n",
    "transcripts_dict = {}\n",
    "for file in files:\n",
    "    file_id = file.split(\"/\")[-1].replace(\"txt\",\"\")\n",
    "    with open(file,'r') as f:\n",
    "        transcripts_dict[file_id] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_csv = pd.read_csv(\"../../data/train_ReAnnotatedData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "def compute_wer(predictions, references):\n",
    "    \"\"\"https://huggingface.co/spaces/evaluate-metric/wer\"\"\"\n",
    "    wer = load(\"wer\")\n",
    "    wer_score = wer.compute(predictions=predictions, references=references)\n",
    "    return wer_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whisper_tiny_int8_beam1': 0.6451211932877564, 'whisper_tiny_int8_beam5': 0.5922933499067744, 'whisper_tiny_int8_beam10': 0.6146674953387197, 'whisper_tiny_float16_beam1': 0.6451211932877564, 'whisper_tiny_float16_beam5': 0.642013673088875, 'whisper_tiny_float16_beam10': 0.6140459912989434, 'whisper_base_int8_beam1': 0.5922933499067744, 'whisper_base_int8_beam5': 0.49720323182100684, 'whisper_base_int8_beam10': 0.5201988812927284, 'whisper_base_float16_beam1': 0.5773772529521441, 'whisper_base_float16_beam5': 0.584213797389683, 'whisper_base_float16_beam10': 0.4916096954630205, 'whisper_medium_int8_beam1': 0.5102548166563082, 'whisper_medium_int8_beam5': 0.5015537600994406, 'whisper_medium_int8_beam10': 0.5021752641392169, 'whisper_medium_float16_beam1': 0.512119328775637, 'whisper_medium_float16_beam5': 0.48166563082660035, 'whisper_medium_float16_beam10': 0.4742075823492853, 'whisper_large-v3_int8_beam1': 0.4592914853946551, 'whisper_large-v3_int8_beam5': 0.4481044126786824, 'whisper_large-v3_int8_beam10': 0.4555624611559975}\n"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_csv(\"../../data/results/test-large/transcriptions_filtered.csv\")\n",
    "\n",
    "eval_res = pd.read_csv(\"../../data/results/test-large/evaluation_results.csv\")\n",
    "\n",
    "ref_col = test_csv[\"rewritten_text\"]\n",
    "\n",
    "res = {}\n",
    "for index, row in eval_res.iterrows():\n",
    "    name = \"_\".join([\"whisper\",row[\"model\"], row[\"compute_type\"],\"beam\"+str(row[\"beam_size\"])])\n",
    "    col_res = test_csv[name]\n",
    "    score = compute_wer(predictions=col_res, references=ref_col)\n",
    "    res[name] = score\n",
    "    \n",
    "print(res)\n",
    "\n",
    "eval_res[\"wer\"] = res.values()\n",
    "# eval_res.to_csv(\"../../data/results/test-large/eval_results_wer.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test WER 0.46424625098658245\n",
      "train WER 0.44568307407289104\n"
     ]
    }
   ],
   "source": [
    "# Compute WER on existing test_ReannotatedData with generated transcripts\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "test_res_df = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/test_ReAnnotatedData.csv\")\n",
    "train_res_df = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/train_ReAnnotatedData.csv\")\n",
    "\n",
    "test_res_df = test_res_df[~test_res_df[\"whisper-largev3\"].isna()]\n",
    "train_res_df = train_res_df[~train_res_df[\"whisper-largev3\"].isna()]\n",
    "\n",
    "def compute_wer(predictions, references):\n",
    "    \"\"\"https://huggingface.co/spaces/evaluate-metric/wer\"\"\"\n",
    "    wer = load(\"wer\")\n",
    "    wer_score = wer.compute(predictions=predictions, references=references)\n",
    "    return wer_score\n",
    "\n",
    "print(f\"test WER {compute_wer(test_res_df[\"whisper-largev3\"],test_res_df[\"rewritten_text\"])}\")\n",
    "print(f\"train WER {compute_wer(train_res_df[\"whisper-largev3\"],train_res_df[\"rewritten_text\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Disinfo/False/Misleading (0/1/2)\n",
       "0    46\n",
       "2    30\n",
       "1    16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_df = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/test_ReAnnotatedData.csv\")\n",
    "train_res_df = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/train_ReAnnotatedData.csv\")\n",
    "\n",
    "test_res_df = test_res_df[~test_res_df[\"whisper-largev3\"].isna()]\n",
    "train_res_df = train_res_df[~train_res_df[\"whisper-largev3\"].isna()]\n",
    "\n",
    "test_res_df[\"Not Disinfo/False/Misleading (0/1/2)\"].value_counts()\n",
    "train_res_df[\"Not Disinfo/False/Misleading (0/1/2)\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diarization\n",
    "\n",
    "Using pyannot https://github.com/pyannote/pyannote-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "/home/cletch/Documents/dataforgood/climateguard/.venv/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0.0s stop=11.7s speaker_SPEAKER_00\n",
      "start=0.6s stop=1.9s speaker_SPEAKER_01\n",
      "start=12.4s stop=26.0s speaker_SPEAKER_00\n",
      "start=13.5s stop=15.6s speaker_SPEAKER_01\n",
      "start=18.9s stop=20.4s speaker_SPEAKER_01\n",
      "start=20.8s stop=21.5s speaker_SPEAKER_01\n",
      "start=22.2s stop=24.0s speaker_SPEAKER_01\n",
      "start=26.5s stop=29.5s speaker_SPEAKER_00\n",
      "start=29.9s stop=37.8s speaker_SPEAKER_00\n",
      "start=33.8s stop=34.7s speaker_SPEAKER_01\n",
      "start=35.1s stop=41.0s speaker_SPEAKER_01\n",
      "start=40.5s stop=40.6s speaker_SPEAKER_00\n",
      "start=40.9s stop=57.3s speaker_SPEAKER_00\n",
      "start=50.0s stop=50.6s speaker_SPEAKER_01\n",
      "start=50.9s stop=51.7s speaker_SPEAKER_01\n",
      "start=59.4s stop=68.2s speaker_SPEAKER_01\n",
      "start=68.6s stop=77.1s speaker_SPEAKER_01\n",
      "start=77.8s stop=82.2s speaker_SPEAKER_01\n",
      "start=82.8s stop=86.8s speaker_SPEAKER_01\n",
      "start=87.8s stop=89.2s speaker_SPEAKER_01\n",
      "start=90.8s stop=92.7s speaker_SPEAKER_01\n",
      "start=93.7s stop=96.1s speaker_SPEAKER_01\n",
      "start=97.0s stop=97.2s speaker_SPEAKER_01\n",
      "start=98.6s stop=99.6s speaker_SPEAKER_01\n",
      "start=100.1s stop=104.3s speaker_SPEAKER_01\n",
      "start=104.7s stop=106.0s speaker_SPEAKER_01\n",
      "start=112.0s stop=120.0s speaker_SPEAKER_01\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=)\n",
    "\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "pipeline.to(torch.device(\"cpu\"))\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(first_file)\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "# start=0.2s stop=1.5s speaker_0\n",
    "# start=1.8s stop=3.9s speaker_1\n",
    "# start=4.2s stop=5.7s speaker_0\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "129\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_annotated = pd.read_csv(\n",
    "    \"../../data/results/transcripts-large-v3-int8-beam5/test.csv\"\n",
    ").id.values.tolist()\n",
    "train_annotated = pd.read_csv(\n",
    "    \"../../data/results/transcripts-large-v3-int8-beam5/train.csv\"\n",
    ").id.values.tolist()\n",
    "already_whispered_id = set(train_annotated).union(set(test_annotated))\n",
    "\n",
    "# test_dir = \"../../data/MediatreeMP3/UpdatedDatasetsMP3/newTest_Audio\"\n",
    "train_dir = \"../../data/MediatreeMP3/UpdatedDatasetsMP3/newTrain_Audio\"\n",
    "\n",
    "audio_files = [file.split(\".\")[0] for file in os.listdir(train_dir) if file.endswith(\".mp3\")]\n",
    "# audio_files.extend([file.split(\".\")[0] for file in os.listdir(train_dir) if file.endswith(\".mp3\")])\n",
    "audio_files = set(audio_files)\n",
    "\n",
    "print(len(audio_files))\n",
    "print(len(already_whispered_id))\n",
    "\n",
    "print(len(audio_files - already_whispered_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_files.intersection(already_whispered_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 230\n",
      "test: 79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"../../data/climatesafeguards/train_230.csv\")\n",
    "test_df = pd.read_csv(\"../../data/climatesafeguards/test_79.csv\")\n",
    "\n",
    "print(f\"train: {len(train_df)}\")\n",
    "print(f\"test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_audio_path = \"../../data/MediatreeMP3/UpdatedDatasetsMP3/newTrain_Audio\"\n",
    "test_audio_files = [file for file in os.listdir(test_audio_path) if file.endswith(\".mp3\")]\n",
    "print(len(test_audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "217\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../../data/test_ReAnnotatedData.csv\")\n",
    "print(len(test_df))\n",
    "train_df = pd.read_csv(\"../../data/train_ReAnnotatedData.csv\")\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "122\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_audio_path = \"../../data/MediatreeMP3/UpdatedDatasetsMP3/newTest_Audio\"\n",
    "\n",
    "test_audio_files = [file.split(\".\")[0] for file in os.listdir(test_audio_path) if file.endswith(\".mp3\")]\n",
    "print(len(test_audio_files))\n",
    "\n",
    "train_audio_path = \"../../data/MediatreeMP3/UpdatedDatasetsMP3/newTrain_Audio\"\n",
    "\n",
    "train_audio_files = [file.split(\".\")[0] for file in os.listdir(train_audio_path) if file.endswith(\".mp3\")]\n",
    "print(len(train_audio_files))\n",
    "\n",
    "print(len(test_audio_files) + len(train_audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrain = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/newTrain.csv\")\n",
    "newtest = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/newTest.csv\")\n",
    "\n",
    "test = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/test.csv\")\n",
    "train = pd.read_csv(\"../../data/results/transcripts-large-v3-int8-beam5/train.csv\")\n",
    "\n",
    "all_transcripts = pd.concat([newTrain, newtest, test, train])\n",
    "all_transcripts.rename(columns={\"transcript\":\"whisper-largev3\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts.drop_duplicates(subset=\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "182\n",
      "untranscribed: 0\n"
     ]
    }
   ],
   "source": [
    "audio_ids = set(train_audio_files) # to transcribe\n",
    "all_transcripts_ids = set(all_transcripts.id.values.tolist()) # transcribed\n",
    "\n",
    "print(len(audio_ids))\n",
    "print(len(all_transcripts_ids))\n",
    "print(f\"untranscribed: {len(audio_ids) - len(audio_ids.intersection(all_transcripts_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_transcripts))\n",
    "len(all_transcripts.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTest = pd.read_csv(\"../../data/climatesafeguards/test_79.csv\")\n",
    "newTrain = pd.read_csv(\"../../data/climatesafeguards/train_230.csv\")\n",
    "\n",
    "newTest_whisper = pd.merge(left=newTest, right=all_transcripts, on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(newTest))\n",
    "print(len(newTest_whisper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  transcript\n",
       "0  abc           0\n",
       "1  def           1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\"abc\":0, \"def\":1}\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"transcript\"]).reset_index().rename(columns={\"index\":\"id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTest_whisper.to_csv(\"../../data/results/transcripts-large-v3-int8-beam5/newTest_whisper.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
