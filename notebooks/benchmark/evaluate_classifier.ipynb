{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up an evaluation script with a random classifier #8\n",
    "\n",
    "Create a script that:\n",
    "\n",
    "- Loads the dataset from `Initialize a dataset to evaluate the detection pipeline` #6\n",
    "- Loads the predictions from a random classifier to classify claims using the taxonomy from `Define our contrarian claims taxonomy` #7\n",
    "- Generates a text classification report\n",
    "- Generates a confusion matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateClassifier:\n",
    "\n",
    "    def __init__(self, classifier):\n",
    "        \"\"\"\n",
    "        Initialize the class with a fitted classifier passed as an argument.\n",
    "        :param classifier: A classification model taking a serie of claims as input \n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        self._validate_classifier_input()\n",
    "\n",
    "    def _validate_classifier_input(self):\n",
    "        \"\"\"\n",
    "        Validate that the classifier's `predict` method can accept a list of strings as input.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sample input to test if the classifier accepts a list of strings\n",
    "            sample_input = [\"sample claim 1\", \"sample claim 2\"]\n",
    "            self.classifier.predict(sample_input)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"The classifier's `predict` method must accept a list of strings as input. \"\n",
    "                             \"Ensure the classifier is compatible with text data.\") from e\n",
    "    \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load data from a csv file.\n",
    "\n",
    "        :param file_path: Path to the Excel file containing the data\n",
    "        :return: DataFrame with the loaded data\n",
    "        \"\"\"\n",
    "        benchmark = pd.read_csv(file_path, sheet_name=\"benchmark\")\n",
    "        if 'claim' not in benchmark.columns or 'label' not in benchmark.columns:\n",
    "            raise ValueError(\"Columns 'claim' and 'label' must be present in the benchmark\")\n",
    "        return benchmark\n",
    "    \n",
    "    def predict(self, benchmark):\n",
    "        \"\"\"\n",
    "        Predict classes on the benchmark.\n",
    "\n",
    "        :param X_test: Test features\n",
    "        :return: Classifier predictions\n",
    "        \"\"\"\n",
    "        claims = benchmark['claim']\n",
    "        return self.classifier.predict(claims)\n",
    "    \n",
    "    def generate_classification_report(self, benchmark, y_pred):\n",
    "        \"\"\"\n",
    "        Generate a classification report.\n",
    "\n",
    "        :param y_test: True labels\n",
    "        :param y_pred: Predictions\n",
    "        :return: DataFrame of the classification report\n",
    "        \"\"\"\n",
    "        report = classification_report(benchmark['label'], y_pred, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        print(\"Classification Report:\\n\", report_df)\n",
    "        return report_df\n",
    "\n",
    "    def plot_confusion_matrix(self, y, y_pred, labels):\n",
    "        \"\"\"\n",
    "        Generate and display a confusion matrix.\n",
    "\n",
    "        :param y_test: True labels\n",
    "        :param y_pred: Predictions\n",
    "        :param categories: List of classification categories\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(y, y_pred, labels=labels)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel(\"Predicted Categories\")\n",
    "        plt.ylabel(\"True Categories\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self, file_path):\n",
    "        \"\"\"\n",
    "        Run the complete evaluation: loading benchmark, predicting  with classifier, and evaluating.\n",
    "\n",
    "        :param file_path: Path to the csv file containing the benchmark\n",
    "        \"\"\"\n",
    "        # Load the data\n",
    "        benchmark = self.load_data(file_path)\n",
    "        y = benchmark['label']\n",
    "\n",
    "        # Predict\n",
    "        y_pred = self.predict(benchmark)\n",
    "        labels = self.unique(self.y)\n",
    "\n",
    "        # Generate the classification report\n",
    "        self.generate_classification_report(y, y_pred)\n",
    "\n",
    "        # Generate and display the confusion matrix\n",
    "        self.plot_confusion_matrix(y, y_pred, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a random classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_path = \"../data/benchmark/cards_sample_1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mClaims\u001b[39;00m(BaseModel):\n\u001b[1;32m     17\u001b[0m     claims: \u001b[38;5;28mlist\u001b[39m[Claim]\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mClassifier\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mClassifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muser\u001b[39m(content: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: content }\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_completion\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m---> 32\u001b[0m     messages: List[\u001b[43mDict\u001b[49m],\n\u001b[1;32m     33\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_MODEL,\n\u001b[1;32m     34\u001b[0m     temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m     35\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     38\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     39\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     40\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     41\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "## !!!!!to hide!!!!!!\n",
    "os.environ[\"GROQ_API_KEY\"] = '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'\n",
    "LLAMA3_70B_INSTRUCT = \"llama-3.1-70b-versatile\"\n",
    "DEFAULT_MODEL = LLAMA3_70B_INSTRUCT\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    disinformation_score: str\n",
    "    classification: str\n",
    "    \n",
    "class Claims(BaseModel):\n",
    "    claims: list[Claim]\n",
    "\n",
    "class Classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = Groq()\n",
    "\n",
    "    def assistant(content: str):\n",
    "        return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "    def user(content: str):\n",
    "        return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict],\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "\n",
    "    def completion(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> str:\n",
    "        return self.chat_completion(\n",
    "            [self.user(prompt)],\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "    def classifier(\n",
    "        self,\n",
    "        prompt: str, \n",
    "        model: str = DEFAULT_MODEL\n",
    "        )-> str:\n",
    "        response = self.completion(prompt, model)\n",
    "        label = response[0]['classification']\n",
    "        return label\n",
    "\n",
    "    def predict(self, row):\n",
    "        prompt = f\"\"\"\n",
    "            Tu es expert en désinformation sur les sujets environnementaux, expert en science climatique et sachant tout sur le GIEC. Je vais te donner un extrait d'une retranscription de 2 minutes d'un flux TV ou Radio. \n",
    "            A partir de cet extrait liste moi tous les faits/opinions environnementaux (claim) uniques qu'il faudrait factchecker. Et pour chaque claim, donne une première analyse si c'est de la désinformation ou non, un score si c'est de la désinformation, ainsi qu'une catégorisation de cette allégation.\n",
    "            Ne sélectionne que les claims sur les thématiques environnementales (changement climatique, transition écologique, énergie, biodiversité, pollution, pesticides, ressources (eau, minéraux, ..) et pas sur les thématiques sociales et/ou économiques\n",
    "            Renvoie le résultat en json sans autre phrase d'introduction ou de conclusion avec à chaque fois les champs suivants : \n",
    "\n",
    "            - \"disinformation_score\" - le score de désinformation (voir plus bas)\n",
    "            - \"classification\" - la classification du type de désinformation suivant la taxonomie CARDS (voir plus bas)\n",
    "\n",
    "            Pour les scores \"disinformation_score\"\n",
    "            - \"very low\" = pas de problème, l'allégation n'est pas trompeuse ou à risque. pas besoin d'investiguer plus loin\n",
    "            - \"low\" = allégation qui nécessiterait une vérification et une interrogation, mais sur un sujet peu important et significatif dans le contexte des enjeux écologiques (exemple : les tondeuses à gazon, \n",
    "            - \"medium\" = allégation problématique sur un sujet écologique important (scientifique, impacts, élections, politique, transport, agriculture, énergie, alimentation, démocratie ...) , qui nécessiterait vraiment d'être vérifiée, déconstruite, débunkée et interrogée. En particulier pour les opinions fallacieuses\n",
    "            - \"high\" = allégation grave, en particulier si elle nie le consensus scientifique\n",
    "\n",
    "            Pour la \"classification\":\n",
    "            - \"0\" = \"disinformation_score\" est \"very low\" ou \"low\" \n",
    "            - \"1\": \"climate change is not happening\"\n",
    "            - \"2\": \"climate change is not caused by humans\"\n",
    "            - \"3\": \"the consequences of climate change are not bad\"\n",
    "            - \"4\": \"the solutions won't work\"\n",
    "            - \"5\": \"climate science is unreliable\"\n",
    "\n",
    "            <transcription>\n",
    "            {row['claim']}\n",
    "            </transcription>\n",
    "                \"\"\"\n",
    "        try:\n",
    "            return self.classifier(prompt)\n",
    "        except:\n",
    "            return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = benchmark.apply(lambda row: class_from_prompt(row), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
